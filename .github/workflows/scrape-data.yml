name: Scrape Richmond Data
on:
  workflow_dispatch: # Allows you to run it manually
  schedule:
    - cron: "*/5 * * * *" # Runs every 5 minutes

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          python scrape.py # This creates the local incidents.json

      - name: Upload to Azure Blob Storage
        uses: Azure/storage-blob-upload-action@v2
        with:
          connection-string: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          container-name: "data" # The container name you created in Azure
          source-path: "incidents.json" # The file the scrape.py just made
          destination-path: "incidents.json" # The name of the file in the cloud
          content-type: "application/json" # Important for the browser
          cache-control: "max-age=120" # Tells browsers to cache for 2 min
